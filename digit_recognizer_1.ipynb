{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "\n",
    "train = pd.read_csv('./datasets/train.csv')\n",
    "test = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "#y_train = np.array(df['label'].values)\n",
    "#print(y_train)\n",
    "\n",
    "X_train = (train.iloc[:,1:].values.astype('float32'))\n",
    "y_train = (train.iloc[:,0].values.astype('int32'))\n",
    "y_train = np.tile(y_train, (1, 1))\n",
    "X_test = test.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.m = X_train.shape[0]\n",
    "        self.input_layer_size = X_train.shape[1]\n",
    "        self.hidden_layer_size = 25\n",
    "        self.output_layer_size = 10\n",
    "        self.epsilon = []\n",
    "        self.theta1 = 0\n",
    "        self.theta2 = 0\n",
    "        self.J = 0\n",
    "        self.grad = 0\n",
    "        self.num_labels = 10\n",
    "        self.a1 = 0\n",
    "        self.a2 = 0\n",
    "        self.a3 = 0\n",
    "        \n",
    "    def randInit(self, X, y):\n",
    "        ils = self.input_layer_size\n",
    "        hls = self.hidden_layer_size\n",
    "        ols = self.output_layer_size\n",
    "        \n",
    "        L_in = ils\n",
    "        L_out = hls\n",
    "        \n",
    "        self.epsilon.append(math.sqrt(6) / math.sqrt(L_in + L_out))\n",
    "        \n",
    "        L_in = hls\n",
    "        L_out = ols\n",
    "        \n",
    "        self.epsilon.append(math.sqrt(6) / math.sqrt(L_in + L_out))\n",
    "        \n",
    "        #The theta values are of the form S(j + 1) x S(j) + 1 where S(j) i sthe size of the layer at j\n",
    "        \n",
    "        self.theta1 = np.dot(np.random.rand(hls, ils + 1), (2 * self.epsilon[0])) - self.epsilon[0]\n",
    "        \n",
    "        self.theta2 = np.dot(np.random.rand(ols, hls + 1), (2 * self.epsilon[1])) - self.epsilon[1]\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return ( 1/(1 + np.exp(-z)))\n",
    "    \n",
    "    def sigmoidPrime(self, z):\n",
    "        return np.multiply(z, (1-z))\n",
    "    \n",
    "    def costFunction(self):\n",
    "        X = self.X_train\n",
    "        y = self.y_train\n",
    "        y = y.T\n",
    "        \n",
    "        #Foward propogation\n",
    "        a1 = np.hstack((np.ones((self.m, 1)), X ))\n",
    "        self.a1 = a1\n",
    "        \n",
    "        z2 = np.dot(a1, np.transpose(self.theta1))\n",
    "        \n",
    "        a2 = self.sigmoid(z2)\n",
    "        a2 = np.hstack((np.ones((a2.shape[0], 1)), a2))\n",
    "        self.a2 = a2\n",
    "        \n",
    "        z3 = np.dot(a2, np.transpose(self.theta2))\n",
    "        \n",
    "        a3 = self.sigmoid(z3)\n",
    "        self.a3 = a3\n",
    "        \n",
    "        #Cost function\n",
    "        \n",
    "        numLabels_temp = np.tile([i for i in range(10)], (self.m, 1) )\n",
    "        \n",
    "        yMatrix_temp = np.tile(y, (1,self.num_labels) )\n",
    "        \n",
    "        yMatrix = np.equal(numLabels_temp, yMatrix_temp)\n",
    "        \n",
    "        cost_no_reg = np.sum(np.multiply(yMatrix, np.log(a3)) + np.multiply((1-yMatrix), np.log(1 - a3)))\n",
    "        \n",
    "        lam = 0\n",
    "        \n",
    "        reg = (lam / (2*self.m)) * ((np.sum(np.square(self.theta1[:, 2:None])) + np.sum(np.square(self.theta2[:, 2:None])))) ** 2\n",
    "        #Also why is it 2:None what does that None do exactly, you\n",
    "        #seem to be squaring it twice also since you have np.square and \n",
    "        #Also I think you should be going from 1: it's 0 we want to avoid because that will\n",
    "        #have the bias unit\n",
    "        #then **2 at the end \n",
    "        reg_temp_Keanu = (lam / (2*self.m)) * (np.sum(np.sum(self.theta1[:,1:]**2)) +\\\n",
    "                                               np.sum(np.sum(self.theta2[:,1:]**2)))#Keanu\n",
    "        J =  -(1.0/self.m) * (cost_no_reg + reg)#Keanu\n",
    "        #For the cost function I think you need to multiply it by a\n",
    "        #that negative number to get it positive\n",
    "        \n",
    "        print(J)\n",
    "        \n",
    "        return J\n",
    "        \n",
    "    def backProp(self):\n",
    "        X = self.X_train\n",
    "        y = self.y_train\n",
    "        y = y.T\n",
    "        lam = 0\n",
    "        \n",
    "        numLabels_temp = np.tile([i for i in range(10)], (self.m, 1) )\n",
    "        \n",
    "        yMatrix_temp = np.tile(y, (1,self.num_labels) )\n",
    "        \n",
    "        yMatrix = np.equal(numLabels_temp, yMatrix_temp)\n",
    "        \n",
    "        delta2 = 0\n",
    "        delta1 = 0\n",
    "        \n",
    "        for i in range(0, self.m):\n",
    "            \n",
    "            a1_t = self.a1[i,]\n",
    "            #I think here you're adding two set's of one's what,\n",
    "            #what I've seen you have to add it after you do the dot \n",
    "            #product you might want to use a new variable\n",
    "            #because you're just going to be using parts of a1\n",
    "            #and you don't need to transpose it\n",
    "            a2_t = self.a2[i,].T\n",
    "            a3_t = self.a3[i,].T\n",
    "        \n",
    "            z2_t = np.dot(a1_t, self.theta1.T)\n",
    "            #I'm not sure where you got the sigmoid function for here\n",
    "            #\n",
    "\n",
    "            \n",
    "            yMatrix_i = yMatrix[i,].T\n",
    "            \n",
    "            delta3_i = (a3_t -  yMatrix_i)\n",
    "            z2_t = np.insert(z2_t, 0, 1, axis=0)\n",
    "            \n",
    "            g2 = self.sigmoidPrime(z2_t)\n",
    "            \n",
    "            delta2_i = np.multiply(np.dot(self.theta2.T, delta3_i), g2)#hmm\n",
    "            delta2_i = delta2_i[1:]\n",
    "            \n",
    "            delta2 = delta2 + np.outer(delta3_i.T, a2_t)\n",
    "            \n",
    "            delta1 = delta1 + np.outer(delta2_i.T, a1_t.T)\n",
    "            \n",
    "        theta1_grad = delta1 / self.m\n",
    "        theta2_grad = delta2 / self.m\n",
    "        \n",
    "        theta1_grad_unreg = np.copy(theta1_grad)\n",
    "        theta2_grad_unreg = np.copy(theta2_grad)\n",
    "        \n",
    "        #I'm not sure that's how you get no bias\n",
    "        \n",
    "        theta1_grad += (float(lam/self.m) * self.theta1)\n",
    "        theta2_grad += (float(lam/self.m) * self.theta2)\n",
    "        \n",
    "        theta1_grad[:,0] = theta1_grad_unreg[:,0]\n",
    "        theta2_grad[:,0] = theta2_grad_unreg[:,0]\n",
    "        \n",
    "        print(theta1_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.12391356499474\n",
      "[[ -286.39242865     0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [-3989.27443795     0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [-6176.08278271     0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " ...\n",
      " [ 3361.22696902     0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [ 8483.5692162      0.             0.         ...     0.\n",
      "      0.             0.        ]\n",
      " [ 1279.06969662     0.             0.         ...     0.\n",
      "      0.             0.        ]]\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(X_train, y_train)\n",
    "nn.randInit(X_train, y_train)\n",
    "nn.costFunction()\n",
    "nn.backProp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
